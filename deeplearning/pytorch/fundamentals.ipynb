{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Tensors?\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "Best explanation can be found here: https://www.youtube.com/watch?v=f5liqUk0ZTw\n",
    "\n",
    "Way to create tensor in pytorch is using `torch.tensor()`\n",
    "\n",
    "Ex: Representing scalar as tensor\n",
    "\n",
    "scalar = torch.tensor(7) \n",
    "\n",
    "Then to get the dimension of the tensor, we can use `ndim`\n",
    "\n",
    "Ex: scalar.ndim => we can assume as number of square bracket in the tensors like [[],[]] then ndim is 2 if [] ndim will return 1\n",
    "\n",
    "And shape gives the number of elements in that each dimension if it is like [2,3] then there are 2 vectors with three elements in each\n",
    "\n",
    "Default datatype of tensor is float 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(7)\n",
    "print(scalar, scalar.dtype)\n",
    "print(scalar.ndim)\n",
    "\n",
    "# To convert a scalar tensor to a Python number, we can use the item method\n",
    "scalar.item()\n",
    "vector = torch.tensor([1, 2, 3, 4])\n",
    "print(vector, vector.dtype,vector.ndim, vector.shape)\n",
    "\n",
    "# vector.item() # This will throw an error because vectors are not scalars and hence cannot be converted to Python numbers\n",
    "\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5,1]])\n",
    "print(matrix, matrix.dtype, matrix.ndim, matrix.shape)\n",
    "\n",
    "# Create a 3D tensor\n",
    "tensor_3d = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(tensor_3d, tensor_3d.dtype, tensor_3d.ndim, tensor_3d.shape)\n",
    "\n",
    "\n",
    "# Random question\n",
    "# Even though [[1,2,3],[2,5,2],[6,1,3]] contains three vectors how is it 2 dimensional tensor?\n",
    "\n",
    "# In mathematics and computer science, the number of dimensions refers to the number of indices required to access an element within a data structure.\n",
    "# In the case of a tensor, the number of dimensions indicates how many indices are needed to access individual elements within the tensor.\n",
    "\n",
    "# Let's break down your example tensor [[1,2,3],[2,5,2],[6,1,3]]:\n",
    "\n",
    "# This tensor consists of 3 vectors: [1,2,3], [2,5,2], and [6,1,3].\n",
    "# Each vector contains 3 elements.\n",
    "# When we talk about the number of dimensions of a tensor, we're referring to the number of indices required to access individual elements. In this case:\n",
    "\n",
    "# To access an individual element within the tensor (e.g., element 3 in the first vector), we need two indices: the row index and the column index.\n",
    "# The row index identifies which vector we're accessing, and the column index identifies which element within that vector we're accessing.\n",
    "# Therefore, even though the tensor contains 3 vectors, it's still considered a 2-dimensional tensor because we only need 2 indices (row and column indices) to access individual elements within the tensor.\n",
    "\n",
    "# In summary, the number of dimensions of a tensor is determined by the number of indices required to access individual elements, not by the number of vectors or sub-arrays contained within the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Randomly initialized tensor\n",
    "# Use of random tensors is very common in deep learning as they start leanring by initializing with random numbers. \n",
    "# We can create random tensors using the torch.rand function which creates a tensor with random values that are uniformly distributed between 0 and 1.\n",
    "# And then these gets updated using backpropagation and optimization algorithms like gradient descent, Adam, etc.\n",
    "\n",
    "# Regular cycle of training a model\n",
    "# 1. Initialize the model with random weights\n",
    "# 2. Perform forward pass\n",
    "# 3. Calculate the loss\n",
    "# 4. Perform backpropagation\n",
    "# 5. Update the weights\n",
    "# 6. Repeat steps 2-5\n",
    "\n",
    "\n",
    "random_tensor = torch.rand(3, 4)\n",
    "# This means that the tensor will have 3 rows and 4 columns\n",
    "\n",
    "print(random_tensor, random_tensor.dtype, random_tensor.ndim, random_tensor.shape)\n",
    "\n",
    "test  = torch.tensor([[1,2,3],[4,5,6],[1,2,4]])\n",
    "print(test, test.dtype, test.ndim, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an random image tensor\n",
    "image_tensor = torch.rand(size=(3, 256, 256)) # 3 channels (RGB), 256x256 pixels\n",
    "\n",
    "print(image_tensor, image_tensor.dtype, image_tensor.ndim, image_tensor.shape)\n",
    "\n",
    "# Explantion of the above tensor\n",
    "# The tensor represents a 256x256 image with 3 color channels (red, green, and blue).\n",
    "# Each pixel in the image is represented by a 3-dimensional vector containing the intensity of each color channel.\n",
    "# The tensor has 3 dimensions: the first dimension represents the color channels which is tensor[0] is red, tensor[1] is green and tensor[2] is blue\n",
    "# , the second dimension represents the rows of pixels, and the third dimension represents the columns of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And to see the image we can use the matplotlib library\n",
    "plt.imshow(image_tensor[0], cmap='gray') \n",
    "plt.show()\n",
    "\n",
    "# create a tensor with red pixels as 0\n",
    "# in this case we are setting the red channel to 0, which means that the image will have no red color\n",
    "image_tensor[0] = 0\n",
    "plt.imshow(image_tensor[0], cmap='gray')\n",
    "\n",
    "plt.imshow(image_tensor[1], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating a tensor with all zeros, we can use the torch.zeros function\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "print(zeros_tensor)\n",
    "\n",
    "# For creating a tensor with all ones, we can use the torch.ones function\n",
    "ones_tensor = torch.ones(3, 4)\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 most used parameters in torch.tensor\n",
    "# dtype: The data type of the elements within the tensor, e.g., torch.float32, torch.int64, torch.bool.\n",
    "# device: The device (CPU or GPU) on which the tensor should be allocated.\n",
    "# requires_grad: If set to True, it starts tracking all operations on the tensor. This allows you to call .backward() and compute gradients with respect to the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1,2,4])\n",
    "mult = torch.matmul(tensor, tensor)\n",
    "print(mult, mult.dtype, mult.shape, mult.requires_grad)\n",
    "\n",
    "# In PyTorch, torch.matmul() function performs matrix multiplication for tensors of rank 1 (vectors), 2 (matrices), and higher. However, when both operands are 1-dimensional tensors (vectors), torch.matmul() performs an inner product (dot product) instead of matrix multiplication.\n",
    "# Therefore, when you execute torch.matmul(tensor, tensor) with tensor = torch.tensor([1,2,4]), it calculates the dot product of the tensor with itself:\n",
    "# [1, 2, 4] ⋅ [1, 2, 4] = 1*1 + 2*2 + 4*4 = 1 + 4 + 16 = 21   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a min, max, mean, sum of a tensor\n",
    "mat1 = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "mat2 = torch.tensor([[9,8,7],[6,5,4],[3,2,1]])\n",
    "\n",
    "print(mat1.min())\n",
    "# or\n",
    "print(torch.min(mat1))\n",
    "\n",
    "# max\n",
    "print(mat1.max())\n",
    "# or\n",
    "print(torch.max(mat1))\n",
    "\n",
    "# mean\n",
    "# print(mat1.mean()) \n",
    "# this will give an error because the default data type of the mean is float and the tensor is of type int\n",
    "# so we need to convert the tensor to float\n",
    "\n",
    "print(mat1.float().mean())\n",
    "\n",
    "# or other way\n",
    "print(torch.mean(\n",
    "    mat1.type(torch.float)\n",
    "))\n",
    "\n",
    "# sum\n",
    "print(mat1.sum())\n",
    "# or\n",
    "print(torch.sum(mat1))\n",
    "\n",
    "# To find the index of the min and max value\n",
    "print(mat1.argmin())\n",
    "print(mat1.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reshape operation allows you to change the shape of a tensor without changing its data.\n",
    "# The reshape operation is commonly used in deep learning to prepare input data for neural networks.\n",
    "# For example, you may need to reshape an image tensor from a 3D tensor of shape (3, 256, 256) to a 1D tensor of shape (196608) before passing it to a neural network.\n",
    "\n",
    "# To reshape a tensor, you can use the torch.reshape() function or the .reshape() method.\n",
    "# The reshape operation requires that the number of elements in the original tensor matches the number of elements in the reshaped tensor.\n",
    "# In other words, the total number of elements in the original tensor must be the same as the total number of elements in the reshaped tensor.\n",
    "\n",
    "example_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(example_tensor, example_tensor.shape)\n",
    "reshaped_tensor = torch.reshape(example_tensor, (3, 2))\n",
    "print(reshaped_tensor, reshaped_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Method\n",
    "# The view method is another way to reshape a tensor, but it will return a tensor which on changing will also change the original tensor, because it shares the same memory as the original tensor, so it will be more memory efficient.\n",
    "# It is similar to the reshape method but has some additional features.\n",
    "# The view method returns a new tensor with the same data as the original tensor but with a different shape.\n",
    "print(example_tensor, example_tensor.shape)\n",
    "view_tensor = example_tensor.view(3, 2)\n",
    "print(view_tensor, view_tensor.shape)\n",
    "print(example_tensor)\n",
    "\n",
    "# changing view_tensor will also change the example_tensor\n",
    "# Example:\n",
    "view_tensor[0, 0] = 1000\n",
    "print(view_tensor)\n",
    "print(example_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking, Squeezing, Unsequeezing, Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "# Stacking is a common operation in deep learning where you combine multiple tensors into a single tensor.\n",
    "# There are two main ways to stack tensors in PyTorch: torch.cat() and torch.stack().\n",
    "# The torch.cat() function concatenates tensors along a specified dimension, while the torch.stack() function stacks tensors along a new dimension.\n",
    "\n",
    "# torch.cat()\n",
    "# The torch.cat() function concatenates tensors along a specified dimension.\n",
    "# The tensors must have the same shape along the specified dimension in order to be concatenated.\n",
    "# The dimension along which the tensors are concatenated is called the concatenation dimension.\n",
    "\n",
    "example_tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "example_tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "concatenated_tensor = torch.cat((example_tensor1, example_tensor2), dim=1)\n",
    "print(concatenated_tensor)\n",
    " \n",
    "# torch.stack()\n",
    "# The torch.stack() function stacks tensors along a new dimension.\n",
    "# The tensors must have the same shape in order to be stacked.\n",
    "# The new dimension is added at the specified index.\n",
    "\n",
    "stacked_tensor = torch.stack((example_tensor1, example_tensor2), dim=0)\n",
    "print(stacked_tensor)\n",
    "\n",
    "# Unstacking\n",
    "# Unstacking is the process of splitting a tensor into multiple tensors along a specified dimension.\n",
    "# The torch.chunk() function splits a tensor into a specific number of chunks along a specified dimension.\n",
    "# The torch.split() function splits a tensor into chunks of a specified size along a specified dimension.\n",
    "\n",
    "chunks = torch.chunk(example_tensor, 3, dim=0)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squuezing and unsqueezing\n",
    "# Squeezing a tensor removes dimensions of size 1 from the tensor.\n",
    "# Unsqueezing a tensor adds dimensions of size 1 to the tensor.\n",
    "\n",
    "# Squeezing\n",
    "# The torch.squeeze() function removes dimensions of size 1 from a tensor.\n",
    "# By default, it removes all dimensions of size 1 from the tensor.\n",
    "# You can also specify the dimensions to be removed using the dim parameter.\n",
    "\n",
    "tensor = torch.tensor([[[1, 2, 3]]])\n",
    "squeezed_tensor = torch.squeeze(tensor)\n",
    "print(squeezed_tensor)\n",
    "\n",
    "# Unsqueezing\n",
    "# The torch.unsqueeze() function adds dimensions of size 1 to a tensor.\n",
    "# You can specify the position of the new dimensions using the dim parameter.\n",
    "\n",
    "unsqueezed_tensor = torch.unsqueeze(squeezed_tensor, dim=0)\n",
    "# dim 0 will add a new dimension at the 0th position\n",
    "print(unsqueezed_tensor)\n",
    "\n",
    "# dim 1 will add a new dimension at the 1st position\n",
    "unsqueezed_tensor = torch.unsqueeze(squeezed_tensor, dim=1)\n",
    "print(unsqueezed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute\n",
    "# The permute operation changes the order of dimensions in a tensor and this is also same as view method so it will also share the same memory as the original tensor, so on changing the permuted tensor, the original tensor will also change.\n",
    "# It is commonly used in deep learning to prepare input data for neural networks.\n",
    "# For example, you may need to permute the dimensions of an image tensor from (3, 256, 256) to (256, 256, 3) before passing it to a neural network.\n",
    "\n",
    "tensor = torch.rand(size=(3, 256, 256)) # 3 channels (RGB), 256x256 pixels\n",
    "print(tensor.shape)\n",
    "permuted_tensor = torch.permute(tensor, (2, 0, 1)) # change the order of dimensions from (3, 256, 256) to (256, 3, 256)\n",
    "# this means that the 0th dimension will be the 2nd dimension, the 1st dimension will be the 0th dimension and the 2nd dimension will be the 1st dimension\n",
    "#  In this practically what it means is that 256, 3 * 256 matrices will be created, each of those 256 matrices will indicate the 3 channels of the image which are rows, columns is for height\n",
    "print(permuted_tensor.shape)\n",
    "print(permuted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch tensors and numpy arrays\n",
    "# PyTorch tensors and NumPy arrays are interoperable, meaning that you can convert a PyTorch tensor to a NumPy array and vice versa.\n",
    "# This is useful when you want to use a library or function that only accepts NumPy arrays as input, or when you want to use a PyTorch tensor in a function that only accepts NumPy arrays.\n",
    "# To convert a PyTorch tensor to a NumPy array, you can use the .numpy() method.\n",
    "\n",
    "coverted_numpy = tensor.numpy()\n",
    "print(type(coverted_numpy), coverted_numpy)\n",
    "\n",
    "recover_tensor = torch.from_numpy(coverted_numpy)\n",
    "print(type(recover_tensor), recover_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility in PyTorch\n",
    "# Reproducibility refers to the ability to reproduce the same results across multiple runs of a program.\n",
    "# In deep learning, reproducibility is important for debugging, testing, and comparing different models.\n",
    "# To achieve reproducibility in PyTorch, you can set the random seed for the random number generators used by PyTorch and other libraries.\n",
    "\n",
    "# Use Case: If you want to share your code with others or reproduce the results of your experiments, you should set the random seed to ensure that the results are consistent across different runs.\n",
    "\n",
    "# Example\n",
    "random_tensor1 = torch.rand(3, 4)\n",
    "random_tensor2 = torch.rand(3, 4)\n",
    "print(random_tensor1==random_tensor2)\n",
    "\n",
    "# To set the random seed in PyTorch, you can use the torch.manual_seed() function, and 42 is just a random number which is used by convention, you can use any number.\n",
    "torch.manual_seed(42)\n",
    "random_tensor1 = torch.rand(3, 4)\n",
    "torch.manual_seed(42)\n",
    "random_tensor2 = torch.rand(3, 4)\n",
    "# So the above code will return True because the random numbers generated by the two tensors will be the same as the random seed is set to 42.\n",
    "print(random_tensor1==random_tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# The device parameter specifies the device (CPU or GPU) on which the tensor should be allocated.\n",
    "# If the device is set to 'cuda', the tensor will be allocated on the GPU.\n",
    "# If the device is set to 'cpu', the tensor will be allocated on the CPU.\n",
    "# The device parameter can be set when creating a tensor using the torch.tensor() function or by calling the .to() method on an existing tensor.\n",
    "# And one thing to note is that if the tensor in in GPU then we cannot conver the tensor to numpy array, we need to first convert it to CPU and then to numpy array.\n",
    "\n",
    "# Example\n",
    "tensor = torch.tensor([1, 2, 3], device=device)\n",
    "print(tensor)\n",
    "\n",
    "# To move a tensor from one device to another, you can use the .to() method.\n",
    "# The .to() method takes the target device as an argument and returns a new tensor allocated on the target device.\n",
    "\n",
    "# Example\n",
    "tensor = tensor.to('cpu')\n",
    "print(tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
