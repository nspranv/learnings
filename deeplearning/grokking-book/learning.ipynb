{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you want only positive error?<br>\n",
    "Eventually, you’ll be working with millions of input -> goal_prediction pairs, and we’ll still want to make accurate predictions. So, you’ll try to take the average error down to 0. For example, if you have 3 errors: -1, 1, and 2, the average error is 0.66. But if you have 3 errors: 1, 1, and 1, the average error is 1.0. So, you’ll want to make sure that the errors are positive, so that they don’t cancel each other out.\n",
    "\n",
    "What is Hot and cold learning ?<br>\n",
    "Hot and cold learning means wiggling the weights to see which direction reduces the error the most, moving the weights in that direction, and repeating until the error gets to 0.\n",
    "\n",
    "Problems of Hot and cold learning:<br>\n",
    "1. It’s inefficient. You have to wiggle the weights in every direction to see which one reduces the error the most.\n",
    "2. It’s imprecise. You can’t tell how much to wiggle the weights to reduce the error the most. You have to try different amounts to see what works best.\n",
    "3. It’s slow. You have to try different amounts of wiggling for every weight, and you have to do this for every input -> goal_prediction pair.\n",
    "\n",
    "What is Gradient descent?<br>\n",
    "Gradient descent is a way to find the weights that reduce the error the most, without trying all possible weights. It’s a way to find the weights that reduce the error the most for a single input -> goal_prediction pair, and then do this for every input -> goal_prediction pair.\n",
    "\n",
    "What is divergence?<br>\n",
    "Divergence is when the weights get so big that they blow up and the error becomes NaN (not a number). This is because the weights are so big that they cause the output to be so big that it’s not a real number. This is a problem because it means that the weights are too big and need to be made smaller. This is done by multiplying the weights by a number less than 1, called the learning rate. \n",
    "\n",
    "What is learning rate?<br>\n",
    "The learning rate is a number less than 1 that tells you how much to change the weights to reduce the error the most. It’s a way to make sure that the weights don’t get so big that they blow up and the error becomes NaN. It’s a way to make sure that the weights don’t get so small that they don’t change the output. It prevents from overshooting.\n",
    "\n",
    "What does overshooting mean?<br>\n",
    "Overshooting means that the weights get so big that they blow up and the error becomes NaN. This is because the weights are so big that they cause the output to be so big that it’s not a real number. This is a problem because it means that the weights are too big and need to be made smaller. This is done by multiplying the weights by a number less than 1, called the learning rate.\n",
    "\n",
    "<!-- 2nd part of ofc -->\n",
    "Why training more can lead to overfitting?<br>\n",
    "Training a machine learning model for too many epochs or iterations can lead to overfitting due to the model learning the training data too well, including its noise and outliers, at the expense of generalization to unseen data.\n",
    "\n",
    "How do you get neural networks to train only on the signal (the essence of a dog) and ignore the noise (other stuff irrelevant to the classification)?<br>\n",
    "One way is early stopping. It turns out a large amount of noise comes in the fine-grained detail of an image, and most of the signal (for objects) is found in the general shape and perhaps color of the image.\n",
    "\n",
    "Types of regularization techniques:<br>\n",
    "1. Early stopping: Stop training when the validation error starts to increase, indicating that the model is starting to overfit.\n",
    "2. Dropout(Industry Standard): Randomly set a fraction of input units to 0 at each update during training, which helps prevent overfitting.\n",
    "3. Batch Gradient Descent: Update the weights after each batch of training data, rather than after each individual data point, to help prevent overfitting.\n",
    "\n",
    "Contraints for choosing the activation function:<br>\n",
    "1. Must be continuous and differentiable: This is necessary for gradient-based optimization algorithms to work.\n",
    "2. Must be infinite in domain: This allows the network to learn complex functions and approximate any continuous function.\n",
    "3. Good activation functions are monotonic, never changing direction. This helps the network learn more effectively. This particular constraint isn’t technically a requirement. Unlike functions that have missing values (noncontinuous), you can optimize functions that aren’t monotonic. But consider the implication of having multiple input values map to the same output value.\n",
    "4. Good activation functions are nonlinear (they squiggle or turn). This allows the network to learn from data that isn’t linearly separable.\n",
    "\n",
    "State of the art activation functions:<br>\n",
    "1. ReLU: Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning models. It is defined as f(x) = max(0, x) and is computationally efficient.\n",
    "2. Sigmoid: The sigmoid function is a classic activation function that squashes the output to the range [0, 1]. It is often used in the output layer of binary classification models.\n",
    "3. Tanh: The hypebolic tangent (tanh) function is similar to the sigmoid function but squashes the output to the range [-1, 1]. It is often used in the hidden layers of neural networks. This function is zero-centered, which can help with optimization.\n",
    "4. Leaky ReLU: Leaky ReLU is a variant of the ReLU function that allows a small gradient when the input is negative, preventing the dying ReLU problem.\n",
    "5. Softmax: The softmax function is commonly used in the output layer of multi-class classification models to convert raw scores into probabilities.\n",
    "\n",
    "Why do we need softmax activation when we have sigmoid activation?<br>\n",
    "Sigmoid activation is used for binary classification, squashing output values between 0 and 1 for two-class problems. Softmax activation, however, is used for multi-class classification, ensuring the output probabilities sum up to 1 across all classes, allowing the model to make probabilistic predictions across multiple classes. They serve different purposes based on the nature of the classification problem: sigmoid for binary and softmax for multi-class tasks. This softmax activatiob function reduces the effort to penalize the wrong class and makes the right class more probable.\n",
    "\n",
    "When neural networks have lots of parameters but not very many training examples, overfitting is difficult to avoid. What is the solution?<br>\n",
    "Regularization techniques can help prevent overfitting in neural networks with many parameters and limited training examples. Some common regularization techniques include:\n",
    "\n",
    "But even with regularization, the model may still overfit if the training data is limited. In such cases, other strategies can be employed to mitigate overfitting.\n",
    "\n",
    "For example incase of images there are lot of parameters but not very many training examples, overfitting is difficult to avoid. So there is a architecture called Convolutional Neural Networks (CNNs) which are specifically designed to handle image data and have been shown to be effective in learning from limited training examples. CNNs use convolutional layers to extract features from images and pooling layers to reduce the dimensionality of the data, which can help prevent overfitting by capturing the most important information in the images. Basically it reduces the number of parameters and hence the model is less likely to overfit. \n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "In this case lots of linear layers are reused instead of a very big linear layer. This is done by using convolutional layers which are specifically designed to handle image data and have been shown to be effective in learning from limited training examples. CNNs use convolutional layers to extract features from images and pooling layers to reduce the dimensionality of the data, which can help prevent overfitting by capturing the most important information in the images. Basically it reduces the number of parameters and hence the model is less likely to overfit.\n",
    "\n",
    "In CNNs , basically there are filters which are also called as kernels which are used to extract features from the images. These filters are used to convolve(traverse) over the input image and extract features like edges, textures, shapes, etc. These features are then passed through activation functions like ReLU to introduce non-linearity in the model. The output of the convolutional layers is then passed through pooling layers to reduce the dimensionality of the data and make the model more robust to variations in the input. \n",
    "\n",
    "This technique allows each kernel to learn a particular pattern and then search for the existence of that pattern somewhere in the image while inference. This is the reason why CNNs are so effective in image recognition tasks and can also used to overcome the overfitting problem in neural networks with many parameters and limited training examples.\n",
    "\n",
    "\"Reusing weights is one of the most important innovations in deep learning.\" - What does this mean?<br>\n",
    "Reusing weights in deep learning refers to the practice of sharing the same set of weights across multiple parts of a neural network. This allows the network to learn common patterns or features in the data more efficiently by reusing the learned representations across different parts of the network. By sharing weights, the network can generalize better to new data and make more accurate predictions.\n",
    "For example, in Convolutional Neural Networks (CNNs), the same set of filters (weights) is applied to different parts of the input image to extract features such as edges, textures, and shapes. By reusing the same filters, the network can learn to detect these features more effectively and efficiently, leading to better performance on image recognition tasks.\n",
    "\n",
    "#### Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language. NLP techniques enable computers to understand, interpret, and generate human language, allowing for tasks such as language translation, sentiment analysis, text summarization, and more.\n",
    "\n",
    "NLP tries to solve the problem of understanding and generating human language by using techniques from machine learning, deep learning, and linguistics. Some common NLP tasks include:\n",
    "\n",
    "1. Text Classification: Categorizing text documents into predefined categories or labels.\n",
    "2. Named Entity Recognition (NER): Identifying and classifying named entities (e.g., names of people, organizations, locations) in text.\n",
    "3. Sentiment Analysis: Determining the sentiment or opinion expressed in text (e.g., positive, negative, neutral).\n",
    "4. Machine Translation: Translating text from one language to another.\n",
    "5. Text Generation: Generating human-like text based on input prompts.\n",
    "6. Question Answering: Answering questions based on a given context or passage.\n",
    "7. Text Summarization: Generating concise summaries of longer text documents.\n",
    "\n",
    "NLP techniques often involve processing and analyzing text data using methods such as tokenization, part-of-speech tagging, parsing, and semantic analysis. Machine learning models, including neural networks, are commonly used in NLP tasks to learn patterns and relationships in text data and make predictions or generate text.\n",
    "\n",
    "Generally speaking, NLP tasks seek to do one of three things:<br>\n",
    "1. label a region of text\n",
    "2. link two or more regions of text\n",
    "3. try to fill in missing information (missing words) based on context.\n",
    "\n",
    "why do neural networks only take fixed size input's?<br>\n",
    "Neural networks typically require fixed-size inputs because the architecture and parameters of the network are fixed during training and inference. Having fixed-size inputs allows for consistent and efficient computation within the network.\n",
    "\n",
    "When processing data with neural networks, each layer in the network expects inputs of a specific size, and the parameters of the network (such as the number of neurons in each layer) are set accordingly. If inputs were of variable sizes, it would be challenging to ensure that the dimensions of the input match the expected dimensions of each layer, leading to inconsistencies and difficulties in training and inference.\n",
    "\n",
    "Additionally, fixed-size inputs facilitate the use of batch processing techniques, where multiple data samples are processed simultaneously to improve computational efficiency. With fixed-size inputs, batches of data can be easily organized and processed together, optimizing computation and memory usage during training.\n",
    "\n",
    "Furthermore, many neural network architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), rely on specific input shapes and sizes to leverage their inherent structure and properties effectively. For example, CNNs use filters of fixed sizes to extract features from images, while RNNs process sequences of fixed lengths to capture temporal dependencies in data.\n",
    "\n",
    "Overall, requiring fixed-size inputs allows neural networks to maintain consistency in computation, facilitate batch processing, and leverage the specific architectures and properties of different network types for efficient and effective learning and inference.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### RNNs\n",
    "\n",
    "##### what is the need of 1 of k encoding in rnns?<br>\n",
    "In the context of Recurrent Neural Networks (RNNs), \"1 of K\" encoding (also known as \"one-hot encoding\") is a method used to represent categorical variables as binary vectors. Each category is represented by a vector where one element is set to 1 and all other elements are set to 0.\n",
    "\n",
    "The need for \"1 of K\" encoding in RNNs arises when dealing with categorical data as input or output. Here's why it's used:\n",
    "\n",
    "Representation of Categorical Variables: RNNs often deal with sequential data where some of the inputs or outputs are categorical in nature (e.g., words in a sentence, characters in a text, categorical labels). These categorical variables need to be represented numerically for the RNN to process them.\n",
    "\n",
    "Ensuring Numerical Compatibility: Neural networks, including RNNs, typically operate on numerical data. By using \"1 of K\" encoding, categorical variables are converted into numerical representations that can be processed by the network.\n",
    "\n",
    "Distinct Representations: Each category is represented by a unique binary vector, ensuring that the network can distinguish between different categories. This is crucial for tasks such as language modeling, where each word needs to be uniquely represented.\n",
    "\n",
    "Loss Calculation: In many tasks, RNNs are trained using loss functions that require numerical predictions. One-hot encoding facilitates the comparison between the predicted outputs of the network (also in one-hot encoded form) and the ground truth labels.\n",
    "\n",
    "Output Layer Activation: For classification tasks, RNNs often use softmax activation in the output layer, which requires one-hot encoded representations for proper calculation of probabilities across classes.\n",
    "\n",
    "In summary, \"1 of K\" encoding is needed in RNNs to represent categorical variables numerically, ensure compatibility with neural network operations, enable distinct representations for different categories, facilitate loss calculation, and support appropriate activation functions in the output layer.\n",
    "\n",
    "##### But can't we can just use the the position of the character in the vocabulary right ? but are we usign this encoding in RNNs?\n",
    "Yes, you are correct that instead of using \"1 of K\" encoding, one could represent categorical variables, such as characters or words, using their position in a vocabulary. This approach is commonly referred to as \"index-based encoding\" or \"integer encoding.\"\n",
    "\n",
    "In index-based encoding, each unique category (e.g., character or word) is mapped to a unique integer value based on its position in the vocabulary. For example, if we have a vocabulary containing the characters 'a', 'b', 'c', and 'd', we could assign them the integer values 0, 1, 2, and 3 respectively.\n",
    "\n",
    "While index-based encoding is indeed an alternative to \"1 of K\" encoding, there are some considerations to keep in mind:\n",
    "\n",
    "Size of Vocabulary: If the vocabulary is large, using integer encoding may lead to large integer values, which could be less efficient to handle in terms of memory and computation compared to \"1 of K\" encoding, especially if the integers are not contiguous.\n",
    "\n",
    "Sparse Representation: \"1 of K\" encoding results in a sparse representation, where most elements in the encoded vector are zero except for one. This can be advantageous in terms of memory efficiency, especially for large vocabularies, as only one element per vector needs to be stored as non-zero.\n",
    "\n",
    "Network Behavior: Some neural network architectures may perform better or worse with different encoding schemes. For example, some architectures may be more suited to handle \"1 of K\" encoded inputs efficiently, while others may work well with integer encoded inputs.\n",
    "\n",
    "Both encoding schemes have their own advantages and disadvantages, and the choice between them depends on factors such as the size of the vocabulary, the specific requirements of the task, and the characteristics of the neural network architecture being used.\n",
    "\n",
    "And as we do matrix multiplication in RNNs, we use \"1 of K\" encoding to represent categorical variables as binary vectors, which can be efficiently processed through matrix operations in the network. Making zero values for all other elements except one, helps in efficient computation and representation of categorical data in RNNs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
